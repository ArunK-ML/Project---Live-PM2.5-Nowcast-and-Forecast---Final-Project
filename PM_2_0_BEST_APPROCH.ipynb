{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKoYy1BBwg5Ur2pBvuAvzw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArunK-ML/Project---Live-PM2.5-Nowcast-and-Forecast---Final-Project/blob/main/PM_2_0_BEST_APPROCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **__init__**"
      ],
      "metadata": {
        "id": "oQYVdZosPhdE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4EuKg-87PZSm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "8b7e5753-2032-452f-c569-3899d04a36f0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'preprocessing'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-604347415.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0manomaly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manomaly\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect_anomalies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocessing'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from src.preprocessing.preprocessing import clean_data\n",
        "from src.features.features import feature_pipeline\n",
        "from src.anomaly.anomaly import detect_anomalies\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# init\n",
        "\n",
        "\"\"\"\n",
        "src package for PM2.5 Forecasting Project\n",
        "-----------------------------------------\n",
        "Contains all modules for:\n",
        "- Data fetching (OpenAQ / Open-Meteo)\n",
        "- Preprocessing & feature engineering\n",
        "- Anomaly detection\n",
        "- Forecasting (recursive)\n",
        "- Model training\n",
        "\"\"\"\n",
        "__all__ = [\"fetch\", \"preprocessing\", \"features\", \"anomaly\", \"forecasting\", \"train\"]\n",
        "\n",
        "\n",
        "# fetch\n",
        "\n",
        "\n",
        "def fetch_openaq(city=\"Delhi\", hours=72):\n",
        "    \"\"\"Fetch PM2.5 data from OpenAQ API.\"\"\"\n",
        "    end = datetime.utcnow()\n",
        "    start = end - timedelta(hours=hours)\n",
        "    url = f\"https://api.openaq.org/v2/measurements\"\n",
        "    params = {\n",
        "        \"city\": city,\n",
        "        \"parameter\": \"pm25\",\n",
        "        \"date_from\": start.isoformat(timespec=\"seconds\") + \"Z\",\n",
        "        \"date_to\": end.isoformat(timespec=\"seconds\") + \"Z\",\n",
        "        \"limit\": 10000,\n",
        "        \"sort\": \"desc\"\n",
        "    }\n",
        "    r = requests.get(url, params=params)\n",
        "    if r.status_code != 200:\n",
        "        raise Exception(f\"OpenAQ API failed: {r.status_code}\")\n",
        "    results = r.json().get(\"results\", [])\n",
        "    df = pd.DataFrame(results)\n",
        "    if \"date\" in df.columns:\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"date\"].apply(lambda x: x[\"utc\"]))\n",
        "    df = df[[\"timestamp\", \"value\"]].rename(columns={\"value\": \"pm25\"}).dropna()\n",
        "    df.sort_values(\"timestamp\", inplace=True)\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "def fetch_weather(lat=28.6139, lon=77.2090):\n",
        "    \"\"\"Fetch hourly weather data from Open-Meteo API.\"\"\"\n",
        "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
        "    params = {\n",
        "        \"latitude\": lat,\n",
        "        \"longitude\": lon,\n",
        "        \"hourly\": \"temperature_2m,relative_humidity_2m,wind_speed_10m,pressure_msl\",\n",
        "        \"timezone\": \"UTC\"\n",
        "    }\n",
        "    r = requests.get(url, params=params)\n",
        "    if r.status_code != 200:\n",
        "        raise Exception(\"Weather API fetch failed\")\n",
        "    data = r.json()[\"hourly\"]\n",
        "    df = pd.DataFrame(data)\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"time\"])\n",
        "    df.drop(columns=[\"time\"], inplace=True)\n",
        "    return df\n",
        "\n",
        "def merge_datasets(pm_df, weather_df):\n",
        "    \"\"\"Merge PM2.5 with weather data on timestamp.\"\"\"\n",
        "    df = pd.merge_asof(pm_df.sort_values(\"timestamp\"),\n",
        "                       weather_df.sort_values(\"timestamp\"),\n",
        "                       on=\"timestamp\")\n",
        "    return df\n",
        "\n",
        "# preprocessing\n",
        "\n",
        "\n",
        "def clean_data(df):\n",
        "    \"\"\"Clean and preprocess merged dataset.\"\"\"\n",
        "    df = df.copy()\n",
        "    df = df.ffill().bfill()  # fill missing values\n",
        "    df = df.drop_duplicates(subset=[\"timestamp\"])\n",
        "    df = df.set_index(\"timestamp\").resample(\"1H\").mean().interpolate()\n",
        "    return df.reset_index()\n",
        "\n",
        "def remove_outliers(df, cols=None, z_thresh=3):\n",
        "    \"\"\"Remove statistical outliers using z-score.\"\"\"\n",
        "    if cols is None:\n",
        "        cols = [c for c in df.columns if c != \"timestamp\"]\n",
        "    for c in cols:\n",
        "        z = np.abs((df[c] - df[c].mean()) / df[c].std())\n",
        "        df.loc[z > z_thresh, c] = np.nan\n",
        "    return df.ffill().bfill()\n",
        "\n",
        "def save_processed(df, path=\"data/processed/pm25_merged.csv\"):\n",
        "    \"\"\"Save cleaned data to CSV.\"\"\"\n",
        "    df.to_csv(path, index=False)\n",
        "\n",
        "\n",
        "# features\n",
        "\n",
        "\n",
        "\n",
        "def add_time_features(df):\n",
        "    \"\"\"Add time-based features like hour, dayofweek.\"\"\"\n",
        "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df, col=\"pm25\", lags=6):\n",
        "    \"\"\"Create lag features for time series modeling.\"\"\"\n",
        "    for i in range(1, lags + 1):\n",
        "        df[f\"{col}_lag_{i}\"] = df[col].shift(i)\n",
        "    return df\n",
        "\n",
        "def add_rolling_features(df, col=\"pm25\", windows=[3,6,12]):\n",
        "    \"\"\"Add rolling mean and std features.\"\"\"\n",
        "    for w in windows:\n",
        "        df[f\"{col}_roll_mean_{w}\"] = df[col].rolling(w).mean()\n",
        "        df[f\"{col}_roll_std_{w}\"] = df[col].rolling(w).std()\n",
        "    return df\n",
        "\n",
        "def feature_pipeline(df):\n",
        "    \"\"\"Complete feature engineering pipeline.\"\"\"\n",
        "    df = add_time_features(df)\n",
        "    df = add_lag_features(df)\n",
        "    df = add_rolling_features(df)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "# anomaly\n",
        "\n",
        "\n",
        "def detect_anomalies(df, col=\"pm25\"):\n",
        "    \"\"\"Detect anomalies using IsolationForest.\"\"\"\n",
        "    model = IsolationForest(contamination=0.05, random_state=42)\n",
        "    df[\"anomaly_flag\"] = model.fit_predict(df[[col]])\n",
        "    df[\"anomaly_flag\"] = df[\"anomaly_flag\"].map({1: 0, -1: 1})\n",
        "    return df, model\n",
        "\n",
        "# forecasting\n",
        "\n",
        "def recursive_forecast(model, initial_window, steps, feature_fn=None):\n",
        "    \"\"\"\n",
        "    Perform recursive forecasting for `steps` ahead.\n",
        "    - model: trained regressor\n",
        "    - initial_window: recent feature vector (DataFrame)\n",
        "    - feature_fn: optional function to rebuild features each step\n",
        "    \"\"\"\n",
        "    preds = []\n",
        "    window = initial_window.copy()\n",
        "    for _ in range(steps):\n",
        "        X_last = window.iloc[[-1]].drop(columns=[\"timestamp\", \"pm25\"])\n",
        "        y_pred = model.predict(X_last)[0]\n",
        "        preds.append(y_pred)\n",
        "        next_row = window.iloc[[-1]].copy()\n",
        "        next_row[\"pm25\"] = y_pred\n",
        "        next_row[\"timestamp\"] += pd.Timedelta(hours=1)\n",
        "        window = pd.concat([window, next_row]).reset_index(drop=True)\n",
        "        if feature_fn:\n",
        "            window = feature_fn(window)\n",
        "    forecast_df = pd.DataFrame({\n",
        "        \"timestamp\": pd.date_range(window[\"timestamp\"].iloc[-steps-1], periods=steps+1, freq=\"H\")[1:],\n",
        "        \"forecast\": preds\n",
        "    })\n",
        "    return forecast_df\n",
        "\n",
        "# train\n",
        "\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "from preprocessing.preprocessing import clean_data\n",
        "from features.features import feature_pipeline\n",
        "from anomaly.anomaly import detect_anomalies\n",
        "\n",
        "def train_model(data_path=\"data/processed/pm25_merged.csv\", model_out=\"models/model.joblib\"):\n",
        "    df = pd.read_csv(data_path, parse_dates=[\"timestamp\"])\n",
        "    df = clean_data(df)\n",
        "    df, _ = detect_anomalies(df)\n",
        "    df = feature_pipeline(df)\n",
        "\n",
        "    X = df.drop(columns=[\"timestamp\", \"pm25\"])\n",
        "    y = df[\"pm25\"]\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_val)\n",
        "\n",
        "    mae = mean_absolute_error(y_val, preds)\n",
        "    print(f\"Validation MAE: {mae:.3f}\")\n",
        "\n",
        "    joblib.dump(model, model_out)\n",
        "    print(f\"✅ Model saved to {model_out}\")\n",
        "    return model, mae\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# Create the directory structure for the 'src' package\n",
        "os.makedirs(\"src/preprocessing\", exist_ok=True)\n",
        "os.makedirs(\"src/features\", exist_ok=True)\n",
        "os.makedirs(\"src/anomaly\", exist_ok=True)\n",
        "\n",
        "# Create empty __init__.py files to make the directories packages\n",
        "with open(\"src/__init__.py\", \"w\") as f:\n",
        "    pass\n",
        "with open(\"src/preprocessing/__init__.py\", \"w\") as f:\n",
        "    pass\n",
        "with open(\"src/features/__init__.py\", \"w\") as f:\n",
        "    pass\n",
        "with open(\"src/anomaly/__init__.py\", \"w\") as f:\n",
        "    pass\n",
        "\n",
        "# Write the function definitions into the corresponding files\n",
        "with open(\"src/preprocessing/preprocessing.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def clean_data(df):\n",
        "    \\\"\\\"\\\"Clean and preprocess merged dataset.\\\"\\\"\\\"\n",
        "    df = df.copy()\n",
        "    df = df.ffill().bfill()  # fill missing values\n",
        "    df = df.drop_duplicates(subset=[\"timestamp\"])\n",
        "    df = df.set_index(\"timestamp\").resample(\"1H\").mean().interpolate()\n",
        "    return df.reset_index()\n",
        "\n",
        "def remove_outliers(df, cols=None, z_thresh=3):\n",
        "    \\\"\\\"\\\"Remove statistical outliers using z-score.\\\"\\\"\\\"\n",
        "    if cols is None:\n",
        "        cols = [c for c in df.columns if c != \"timestamp\"]\n",
        "    for c in cols:\n",
        "        z = np.abs((df[c] - df[c].mean()) / df[c].std())\n",
        "        df.loc[z > z_thresh, c] = np.nan\n",
        "    return df.ffill().bfill()\n",
        "\n",
        "def save_processed(df, path=\"data/processed/pm25_merged.csv\"):\n",
        "    \\\"\\\"\\\"Save cleaned data to CSV.\\\"\\\"\\\"\n",
        "    df.to_csv(path, index=False)\n",
        "\"\"\")\n",
        "\n",
        "with open(\"src/features/features.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "def add_time_features(df):\n",
        "    \\\"\\\"\\\"Add time-based features like hour, dayofweek.\\\"\\\"\\\"\n",
        "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df, col=\"pm25\", lags=6):\n",
        "    \\\"\\\"\\\"Create lag features for time series modeling.\\\"\\\"\\\"\n",
        "    for i in range(1, lags + 1):\n",
        "        df[f\"{col}_lag_{i}\"] = df[col].shift(i)\n",
        "    return df\n",
        "\n",
        "def add_rolling_features(df, col=\"pm25\", windows=[3,6,12]):\n",
        "    \\\"\\\"\\\"Add rolling mean and std features.\\\"\\\"\\\"\n",
        "    for w in windows:\n",
        "        df[f\"{col}_roll_mean_{w}\"] = df[col].rolling(w).mean()\n",
        "        df[f\"{col}_roll_std_{w}\"] = df[col].rolling(w).std()\n",
        "    return df\n",
        "\n",
        "def feature_pipeline(df):\n",
        "    \\\"\\\"\\\"Complete feature engineering pipeline.\\\"\\\"\\\"\n",
        "    df = add_time_features(df)\n",
        "    df = add_lag_features(df)\n",
        "    df = add_rolling_features(df)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    return df\n",
        "\"\"\")\n",
        "\n",
        "with open(\"src/anomaly/anomaly.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "def detect_anomalies(df, col=\"pm25\"):\n",
        "    \\\"\\\"\\\"Detect anomalies using IsolationForest.\\\"\\\"\\\"\n",
        "    model = IsolationForest(contamination=0.05, random_state=42)\n",
        "    df[\"anomaly_flag\"] = model.fit_predict(df[[col]])\n",
        "    df[\"anomaly_flag\"] = df[\"anomaly_flag\"].map({1: 0, -1: 1})\n",
        "    return df, model\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"./src\")\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    data_path=\"data/processed/pm25_merged.csv\",\n",
        "    model_out=\"models/model.joblib\"\n",
        "):\n",
        "    # Ensure output folder exists\n",
        "    os.makedirs(os.path.dirname(model_out), exist_ok=True)\n",
        "    os.makedirs(\"data/processed\", exist_ok=True)\n",
        "\n",
        "    # Load data\n",
        "    if not os.path.exists(data_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"❌ Data file not found at {data_path}. Please run preprocessing first.\"\n",
        "        )\n",
        "\n",
        "    print(\"📂 Loading data...\")\n",
        "    df = pd.read_csv(data_path, parse_dates=[\"timestamp\"])\n",
        "\n",
        "    # Clean and preprocess\n",
        "    print(\"🧹 Cleaning data...\")\n",
        "    df = clean_data(df)\n",
        "\n",
        "    # Detect anomalies\n",
        "    print(\"⚠️ Running anomaly detection...\")\n",
        "    df, _ = detect_anomalies(df)\n",
        "\n",
        "    # Feature engineering\n",
        "    print(\"🧠 Creating features...\")\n",
        "    df = feature_pipeline(df)\n",
        "\n",
        "    # Split data\n",
        "    X = df.drop(columns=[\"timestamp\", \"pm25\"])\n",
        "    y = df[\"pm25\"]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"🚀 Training model (Gradient Boosting)...\")\n",
        "    model = GradientBoostingRegressor(\n",
        "        n_estimators=200, learning_rate=0.1, random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    preds = model.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, preds)\n",
        "    print(f\"✅ Validation MAE: {mae:.3f}\")\n",
        "\n",
        "    # Save model\n",
        "    joblib.dump(model, model_out)\n",
        "    print(f\"💾 Model saved successfully to: {model_out}\")\n",
        "\n",
        "    return model, mae\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the paths below if needed\n",
        "    train_model(\n",
        "        data_path=\"data/processed/pm25_merged.csv\",\n",
        "        model_out=\"models/model.joblib\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.ensemble import IsolationForest, GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 1️⃣ DATA FETCHING\n",
        "# ==========================================================\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "TZ = timezone.utc\n",
        "LAT, LON = 13.0827, 80.2707   # Chennai (change as needed)\n",
        "HOURS_BACK = 72               # how many past hours of PM2.5\n",
        "RADIUS_KM = 30                # OpenAQ search radius\n",
        "\n",
        "def fetch_openaq_pm25(lat: float, lon: float, hours: int = 168, radius_km: int = 30) -> pd.DataFrame:\n",
        "    end = datetime.now(TZ)\n",
        "    start = end - timedelta(hours=hours)\n",
        "    base = \"https://api.openaq.org/v2/measurements\"\n",
        "    params = {\n",
        "        \"coordinates\": f\"{lat},{lon}\",\n",
        "        \"radius\": int(radius_km * 1000),\n",
        "        \"parameter\": \"pm25\",\n",
        "        \"date_from\": start.isoformat(),\n",
        "        \"date_to\": end.isoformat(),\n",
        "        \"limit\": 10000,\n",
        "        \"sort\": \"desc\",\n",
        "        \"order_by\": \"datetime\",\n",
        "        \"page\": 1,\n",
        "    }\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            r = requests.get(base, params=params, timeout=30)\n",
        "            if r.status_code >= 400:\n",
        "                return pd.DataFrame()\n",
        "            js = r.json()\n",
        "            items = js.get(\"results\", [])\n",
        "            if not items:\n",
        "                break\n",
        "            df = pd.DataFrame(items)\n",
        "            if \"date\" not in df:\n",
        "                break\n",
        "            df = df[[\"date\", \"value\"]]\n",
        "            df[\"datetime\"] = pd.to_datetime(df[\"date\"].apply(lambda d: d.get(\"utc\")), utc=True)\n",
        "            df = df.drop(columns=[\"date\"]).sort_values(\"datetime\")\n",
        "            frames.append(df)\n",
        "            meta = js.get(\"meta\", {})\n",
        "            found = meta.get(\"found\")\n",
        "            page = params[\"page\"]\n",
        "            limit = params[\"limit\"]\n",
        "            if found is None or page * limit >= int(found):\n",
        "                break\n",
        "            params[\"page\"] = page + 1\n",
        "    except Exception:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if not frames:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.concat(frames, ignore_index=True)\n",
        "    df = df[(df[\"datetime\"] >= start) & (df[\"datetime\"] <= end)]\n",
        "    # hourly median across stations\n",
        "    df_hour = (\n",
        "        df.set_index(\"datetime\")\n",
        "          .groupby(pd.Grouper(freq=\"1H\"))[\"value\"]\n",
        "          .median()\n",
        "          .reset_index()\n",
        "          .rename(columns={\"value\": \"pm25\"})\n",
        "    )\n",
        "    return df_hour\n",
        "\n",
        "\n",
        "def fetch_openmeteo_aq_pm25(lat: float, lon: float, hours: int = 168) -> pd.DataFrame:\n",
        "    end = datetime.now(TZ)\n",
        "    start = end - timedelta(hours=hours)\n",
        "    url = (\n",
        "        \"https://air-quality-api.open-meteo.com/v1/air-quality?\"\n",
        "        f\"latitude={lat}&longitude={lon}&hourly=pm2_5&past_days={(hours//24)+1}&timezone=UTC\"\n",
        "    )\n",
        "    try:\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        h = r.json().get(\"hourly\", {})\n",
        "        if not h:\n",
        "            return pd.DataFrame()\n",
        "        df = pd.DataFrame({\"datetime\": h.get(\"time\", []), \"pm25\": h.get(\"pm2_5\", [])})\n",
        "        df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], utc=True)\n",
        "        return df[(df[\"datetime\"] >= start) & (df[\"datetime\"] <= end)].reset_index(drop=True)\n",
        "    except Exception:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    pm25 = fetch_openaq_pm25(LAT, LON, hours=HOURS_BACK, radius_km=RADIUS_KM)\n",
        "    if pm25.empty:\n",
        "        pm25 = fetch_openmeteo_aq_pm25(LAT, LON, hours=HOURS_BACK)\n",
        "\n",
        "\n",
        "\n",
        "def merge_datasets(pm_df, weather_df):\n",
        "    \"\"\"Merge PM2.5 and weather data by timestamp.\"\"\"\n",
        "    print(\"🔗 Merging datasets...\")\n",
        "    df = pd.merge_asof(\n",
        "        pm_df.sort_values(\"timestamp\"),\n",
        "        weather_df.sort_values(\"timestamp\"),\n",
        "        on=\"timestamp\"\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 2️⃣ PREPROCESSING\n",
        "# ==========================================================\n",
        "\n",
        "def clean_data(df):\n",
        "    \"\"\"Clean and preprocess merged dataset.\"\"\"\n",
        "    df = df.copy()\n",
        "    df = df.ffill().bfill()\n",
        "    df = df.drop_duplicates(subset=[\"timestamp\"])\n",
        "    df = df.set_index(\"timestamp\").resample(\"1H\").mean().interpolate()\n",
        "    return df.reset_index()\n",
        "\n",
        "\n",
        "def remove_outliers(df, cols=None, z_thresh=3):\n",
        "    \"\"\"Remove statistical outliers.\"\"\"\n",
        "    if cols is None:\n",
        "        cols = [c for c in df.columns if c != \"timestamp\"]\n",
        "    for c in cols:\n",
        "        z = np.abs((df[c] - df[c].mean()) / df[c].std())\n",
        "        df.loc[z > z_thresh, c] = np.nan\n",
        "    return df.ffill().bfill()\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 3️⃣ FEATURE ENGINEERING\n",
        "# ==========================================================\n",
        "\n",
        "def add_time_features(df):\n",
        "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df, col=\"pm25\", lags=6):\n",
        "    for i in range(1, lags + 1):\n",
        "        df[f\"{col}_lag_{i}\"] = df[col].shift(i)\n",
        "    return df\n",
        "\n",
        "def add_rolling_features(df, col=\"pm25\", windows=[3,6,12]):\n",
        "    for w in windows:\n",
        "        df[f\"{col}_roll_mean_{w}\"] = df[col].rolling(w).mean()\n",
        "        df[f\"{col}_roll_std_{w}\"] = df[col].rolling(w).std()\n",
        "    return df\n",
        "\n",
        "def feature_pipeline(df):\n",
        "    print(\"🧠 Creating time, lag, and rolling features...\")\n",
        "    df = add_time_features(df)\n",
        "    df = add_lag_features(df)\n",
        "    df = add_rolling_features(df)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 4️⃣ ANOMALY DETECTION\n",
        "# ==========================================================\n",
        "\n",
        "def detect_anomalies(df, col=\"pm25\"):\n",
        "    print(\"⚠️ Detecting anomalies (IsolationForest)...\")\n",
        "    model = IsolationForest(contamination=0.05, random_state=42)\n",
        "    df[\"anomaly_flag\"] = model.fit_predict(df[[col]])\n",
        "    df[\"anomaly_flag\"] = df[\"anomaly_flag\"].map({1: 0, -1: 1})\n",
        "    return df\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 5️⃣ TRAINING\n",
        "# ==========================================================\n",
        "\n",
        "def train_model(df, model_out=\"models/model.joblib\"):\n",
        "    \"\"\"Train Gradient Boosting model.\"\"\"\n",
        "    os.makedirs(os.path.dirname(model_out), exist_ok=True)\n",
        "\n",
        "    df = feature_pipeline(df)\n",
        "    X = df.drop(columns=[\"timestamp\", \"pm25\"])\n",
        "    y = df[\"pm25\"]\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    print(\"🚀 Training model (Gradient Boosting)...\")\n",
        "    model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    preds = model.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, preds)\n",
        "    print(f\"✅ Validation MAE: {mae:.3f}\")\n",
        "\n",
        "    joblib.dump(model, model_out)\n",
        "    print(f\"💾 Model saved to {model_out}\")\n",
        "\n",
        "    return model, mae\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 6️⃣ MAIN PIPELINE\n",
        "# ==========================================================\n",
        "\n",
        "def main():\n",
        "    os.makedirs(\"data/processed\", exist_ok=True)\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "    pm_df = fetch_openaq(\"Delhi\", hours=72)\n",
        "    weather_df = fetch_weather()\n",
        "    merged_df = merge_datasets(pm_df, weather_df)\n",
        "\n",
        "    cleaned_df = clean_data(merged_df)\n",
        "    cleaned_df = remove_outliers(cleaned_df)\n",
        "    cleaned_df = detect_anomalies(cleaned_df)\n",
        "\n",
        "    cleaned_df.to_csv(\"data/processed/pm25_merged.csv\", index=False)\n",
        "    print(\"📁 Processed data saved to data/processed/pm25_merged.csv\")\n",
        "\n",
        "    train_model(cleaned_df, model_out=\"models/model.joblib\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "f0VRurdpK5Xs",
        "outputId": "beef7fce-b32b-4296-c898-cd543160f0e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌫️ Fetching PM2.5 data for Delhi...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2521450596.py:19: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  end = datetime.utcnow()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "OpenAQ API failed: 410",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2521450596.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2521450596.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mpm_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_openaq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Delhi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m72\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0mweather_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_weather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweather_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2521450596.py\u001b[0m in \u001b[0;36mfetch_openaq\u001b[0;34m(city, hours)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"OpenAQ API failed: {r.status_code}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: OpenAQ API failed: 410"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from sklearn.ensemble import IsolationForest, GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# ==========================================================\n",
        "# CONFIG\n",
        "# ==========================================================\n",
        "TZ = timezone.utc\n",
        "# Default location (Chennai). Change to desired lat/lon.\n",
        "LAT, LON = 13.0827, 80.2707\n",
        "HOURS_BACK = 72\n",
        "RADIUS_KM = 30\n",
        "\n",
        "# ==========================================================\n",
        "# 1️⃣ DATA FETCHING\n",
        "# ==========================================================\n",
        "\n",
        "def fetch_openaq_pm25(lat: float, lon: float, hours: int = 168, radius_km: int = 30) -> pd.DataFrame:\n",
        "    \"\"\"Fetch PM2.5 measurements from OpenAQ and return hourly median with a 'timestamp' column (UTC).\"\"\"\n",
        "    end = datetime.now(TZ)\n",
        "    start = end - timedelta(hours=hours)\n",
        "    base = \"https://api.openaq.org/v2/measurements\"\n",
        "    params = {\n",
        "        \"coordinates\": f\"{lat},{lon}\",\n",
        "        \"radius\": int(radius_km * 1000),\n",
        "        \"parameter\": \"pm25\",\n",
        "        \"date_from\": start.isoformat(),\n",
        "        \"date_to\": end.isoformat(),\n",
        "        \"limit\": 10000,\n",
        "        \"sort\": \"desc\",\n",
        "        \"order_by\": \"datetime\",\n",
        "        \"page\": 1,\n",
        "    }\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            r = requests.get(base, params=params, timeout=30)\n",
        "            if r.status_code >= 400:\n",
        "                break\n",
        "            js = r.json()\n",
        "            items = js.get(\"results\", [])\n",
        "            if not items:\n",
        "                break\n",
        "            df = pd.DataFrame(items)\n",
        "            if \"date\" not in df:\n",
        "                break\n",
        "            # extract UTC datetime\n",
        "            df[\"timestamp\"] = pd.to_datetime(df[\"date\"].apply(lambda d: d.get(\"utc\")), utc=True)\n",
        "            df = df[[\"timestamp\", \"value\"]].rename(columns={\"value\": \"pm25\"})\n",
        "            frames.append(df)\n",
        "            meta = js.get(\"meta\", {})\n",
        "            found = meta.get(\"found\")\n",
        "            page = params[\"page\"]\n",
        "            limit = params[\"limit\"]\n",
        "            if found is None or page * limit >= int(found):\n",
        "                break\n",
        "            params[\"page\"] = page + 1\n",
        "    except Exception:\n",
        "        frames = []\n",
        "\n",
        "    if not frames:\n",
        "        return pd.DataFrame(columns=[\"timestamp\", \"pm25\"])\n",
        "\n",
        "    df = pd.concat(frames, ignore_index=True)\n",
        "    df = df[(df[\"timestamp\"] >= start) & (df[\"timestamp\"] <= end)]\n",
        "\n",
        "    # hourly median across stations\n",
        "    df_hour = (\n",
        "        df.set_index(\"timestamp\")\n",
        "          .groupby(pd.Grouper(freq=\"1H\"))[\"pm25\"]\n",
        "          .median()\n",
        "          .reset_index()\n",
        "    )\n",
        "    # ensure sorted\n",
        "    df_hour = df_hour.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    return df_hour\n",
        "\n",
        "def fetch_openmeteo_aq_pm25(lat: float, lon: float, hours: int = 168) -> pd.DataFrame:\n",
        "    \"\"\"Fallback: open-meteo air-quality PM2.5 hourly. Returns 'timestamp' and 'pm25' columns (UTC).\"\"\"\n",
        "    end = datetime.now(TZ)\n",
        "    start = end - timedelta(hours=hours)\n",
        "    # past_days param uses days; request a few days to cover hours window\n",
        "    past_days = max(1, (hours // 24) + 1)\n",
        "    url = (\n",
        "        \"https://air-quality-api.open-meteo.com/v1/air-quality?\"\n",
        "        f\"latitude={lat}&longitude={lon}&hourly=pm2_5&past_days={past_days}&timezone=UTC\"\n",
        "    )\n",
        "    try:\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        h = r.json().get(\"hourly\", {})\n",
        "        if not h:\n",
        "            return pd.DataFrame(columns=[\"timestamp\", \"pm25\"])\n",
        "        df = pd.DataFrame({\"timestamp\": h.get(\"time\", []), \"pm25\": h.get(\"pm2_5\", [])})\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
        "        df = df[(df[\"timestamp\"] >= start) & (df[\"timestamp\"] <= end)].reset_index(drop=True)\n",
        "        return df\n",
        "    except Exception:\n",
        "        return pd.DataFrame(columns=[\"timestamp\", \"pm25\"])\n",
        "\n",
        "def fetch_openmeteo_weather(lat: float, lon: float, hours: int = 168) -> pd.DataFrame:\n",
        "    \"\"\"Fetch meteorological hourly features from Open-Meteo (temperature, humidity, windspeed).\"\"\"\n",
        "    end = datetime.now(TZ)\n",
        "    start = end - timedelta(hours=hours)\n",
        "    past_days = max(1, (hours // 24) + 1)\n",
        "    url = (\n",
        "        \"https://api.open-meteo.com/v1/forecast?\"\n",
        "        f\"latitude={lat}&longitude={lon}&hourly=temperature_2m,relativehumidity_2m,windspeed_10m\"\n",
        "        f\"&past_days={past_days}&timezone=UTC\"\n",
        "    )\n",
        "    try:\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        j = r.json().get(\"hourly\", {})\n",
        "        if not j:\n",
        "            return pd.DataFrame(columns=[\"timestamp\"])\n",
        "        df = pd.DataFrame({\n",
        "            \"timestamp\": j.get(\"time\", []),\n",
        "            \"temperature_2m\": j.get(\"temperature_2m\", []),\n",
        "            \"relativehumidity_2m\": j.get(\"relativehumidity_2m\", []),\n",
        "            \"windspeed_10m\": j.get(\"windspeed_10m\", []),\n",
        "        })\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
        "        return df[(df[\"timestamp\"] >= start) & (df[\"timestamp\"] <= end)].reset_index(drop=True)\n",
        "    except Exception:\n",
        "        return pd.DataFrame(columns=[\"timestamp\"])\n",
        "\n",
        "# ==========================================================\n",
        "# 2️⃣ MERGE + PREPROCESSING\n",
        "# ==========================================================\n",
        "\n",
        "def merge_datasets(pm_df: pd.DataFrame, weather_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merge PM2.5 and weather data by timestamp (asof-merge).\n",
        "    If weather_df is empty, returns pm_df with timestamp/pm25 only.\n",
        "    \"\"\"\n",
        "    print(\"🔗 Merging datasets...\")\n",
        "    pm = pm_df.copy()\n",
        "    pm = pm.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    if weather_df is None or weather_df.empty:\n",
        "        return pm\n",
        "    w = weather_df.copy().sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    # asof merge: nearest earlier weather observation for each pm timestamp\n",
        "    merged = pd.merge_asof(pm, w, on=\"timestamp\", direction=\"nearest\", tolerance=pd.Timedelta(\"1H\"))\n",
        "    # if some weather cols are missing due to tolerance, that's okay\n",
        "    return merged\n",
        "\n",
        "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean and preprocess merged dataset. Ensures hourly frequency, interpolates missing values.\"\"\"\n",
        "    print(\"🧹 Cleaning data...\")\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    # ensure timestamp is datetime tz-aware\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
        "    df = df.drop_duplicates(subset=[\"timestamp\"])\n",
        "    df = df.set_index(\"timestamp\").sort_index()\n",
        "    # resample to 1H and take mean for numeric columns\n",
        "    df = df.resample(\"1H\").mean()\n",
        "    # interpolate small gaps, then forward/backward fill remaining\n",
        "    df = df.interpolate(limit=3).ffill().bfill()\n",
        "    df = df.reset_index()\n",
        "    return df\n",
        "\n",
        "def remove_outliers(df: pd.DataFrame, cols=None, z_thresh=3) -> pd.DataFrame:\n",
        "    \"\"\"Remove statistical outliers by z-score (replace with NaN -> interpolate).\"\"\"\n",
        "    print(\"🚫 Removing outliers...\")\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    if cols is None:\n",
        "        cols = [c for c in df.columns if c not in (\"timestamp\",)]\n",
        "    for c in cols:\n",
        "        if c not in df.columns or not np.issubdtype(df[c].dtype, np.number):\n",
        "            continue\n",
        "        std = df[c].std()\n",
        "        mean = df[c].mean()\n",
        "        if std == 0 or np.isnan(std):\n",
        "            continue\n",
        "        z = np.abs((df[c] - mean) / std)\n",
        "        df.loc[z > z_thresh, c] = np.nan\n",
        "    # interpolate and fill\n",
        "    df = df.interpolate(limit=6).ffill().bfill()\n",
        "    return df\n",
        "\n",
        "# ==========================================================\n",
        "# 3️⃣ FEATURE ENGINEERING\n",
        "# ==========================================================\n",
        "\n",
        "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df: pd.DataFrame, col=\"pm25\", lags=6) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for i in range(1, lags + 1):\n",
        "        df[f\"{col}_lag_{i}\"] = df[col].shift(i)\n",
        "    return df\n",
        "\n",
        "def add_rolling_features(df: pd.DataFrame, col=\"pm25\", windows=[3,6,12]) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for w in windows:\n",
        "        df[f\"{col}_roll_mean_{w}\"] = df[col].rolling(w, min_periods=1).mean()\n",
        "        df[f\"{col}_roll_std_{w}\"] = df[col].rolling(w, min_periods=1).std().fillna(0)\n",
        "    return df\n",
        "\n",
        "def feature_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    print(\"🧠 Creating time, lag, and rolling features...\")\n",
        "    df = df.copy()\n",
        "    # ensure pm25 exists\n",
        "    if \"pm25\" not in df.columns:\n",
        "        raise KeyError(\"pm25 column not found in dataframe (needed for feature engineering).\")\n",
        "    df = add_time_features(df)\n",
        "    df = add_lag_features(df)\n",
        "    df = add_rolling_features(df)\n",
        "    # drop rows with NaNs introduced by lagging\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# ==========================================================\n",
        "# 4️⃣ ANOMALY DETECTION\n",
        "# ==========================================================\n",
        "\n",
        "def detect_anomalies(df: pd.DataFrame, col=\"pm25\") -> pd.DataFrame:\n",
        "    print(\"⚠️ Detecting anomalies (IsolationForest)...\")\n",
        "    df = df.copy()\n",
        "    if col not in df.columns or df.empty:\n",
        "        df[\"anomaly_flag\"] = 0\n",
        "        return df\n",
        "    # IsolationForest expects 2D numeric input and no NaNs\n",
        "    X = df[[col]].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "    model = IsolationForest(contamination=0.05, random_state=42)\n",
        "    flags = model.fit_predict(X)\n",
        "    df[\"anomaly_flag\"] = np.where(flags == -1, 1, 0)\n",
        "    return df\n",
        "\n",
        "# ==========================================================\n",
        "# 5️⃣ TRAINING\n",
        "# ==========================================================\n",
        "\n",
        "def train_model(df: pd.DataFrame, model_out=\"models/model.joblib\"):\n",
        "    \"\"\"Train Gradient Boosting model.\"\"\"\n",
        "    os.makedirs(os.path.dirname(model_out), exist_ok=True)\n",
        "\n",
        "    df = df.copy()\n",
        "    df = feature_pipeline(df)\n",
        "\n",
        "    # drop non-feature columns\n",
        "    drop_cols = [\"timestamp\", \"anomaly_flag\"]\n",
        "    X = df.drop(columns=[c for c in drop_cols if c in df.columns] + [\"pm25\"], errors=\"ignore\")\n",
        "    y = df[\"pm25\"]\n",
        "\n",
        "    # simple chronological split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    print(\"🚀 Training model (Gradient Boosting)...\")\n",
        "    model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    preds = model.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, preds)\n",
        "    print(f\"✅ Validation MAE: {mae:.3f}\")\n",
        "\n",
        "    joblib.dump(model, model_out)\n",
        "    print(f\"💾 Model saved to {model_out}\")\n",
        "\n",
        "    return model, mae\n",
        "\n",
        "# ==========================================================\n",
        "# 6️⃣ MAIN PIPELINE\n",
        "# ==========================================================\n",
        "\n",
        "def main():\n",
        "    os.makedirs(\"data/processed\", exist_ok=True)\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "    # Fetch PM2.5 (OpenAQ preferred, fallback open-meteo)\n",
        "    pm_df = fetch_openaq_pm25(LAT, LON, hours=HOURS_BACK, radius_km=RADIUS_KM)\n",
        "    if pm_df.empty:\n",
        "        print(\"No OpenAQ data found — falling back to Open-Meteo air quality.\")\n",
        "        pm_df = fetch_openmeteo_aq_pm25(LAT, LON, hours=HOURS_BACK)\n",
        "\n",
        "    if pm_df.empty:\n",
        "        raise RuntimeError(\"Failed to fetch any PM2.5 data. Check network / API availability.\")\n",
        "\n",
        "    # Fetch weather features (temperature, humidity, wind)\n",
        "    weather_df = fetch_openmeteo_weather(LAT, LON, hours=HOURS_BACK)\n",
        "\n",
        "    merged_df = merge_datasets(pm_df, weather_df)\n",
        "    cleaned_df = clean_data(merged_df)\n",
        "    cleaned_df = remove_outliers(cleaned_df)\n",
        "    cleaned_df = detect_anomalies(cleaned_df)\n",
        "\n",
        "    processed_path = \"data/processed/pm25_merged.csv\"\n",
        "    cleaned_df.to_csv(processed_path, index=False)\n",
        "    print(f\"📁 Processed data saved to {processed_path}\")\n",
        "\n",
        "    # Train model\n",
        "    train_model(cleaned_df, model_out=\"models/model.joblib\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At3FXFkWMRvS",
        "outputId": "199f82a8-14b7-4170-fe2a-e9201df4746c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No OpenAQ data found — falling back to Open-Meteo air quality.\n",
            "🔗 Merging datasets...\n",
            "🧹 Cleaning data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-929650355.py:150: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  merged = pd.merge_asof(pm, w, on=\"timestamp\", direction=\"nearest\", tolerance=pd.Timedelta(\"1H\"))\n",
            "/tmp/ipython-input-929650355.py:165: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  df = df.resample(\"1H\").mean()\n",
            "/tmp/ipython-input-929650355.py:239: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X = df[[col]].fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚫 Removing outliers...\n",
            "⚠️ Detecting anomalies (IsolationForest)...\n",
            "📁 Processed data saved to data/processed/pm25_merged.csv\n",
            "🧠 Creating time, lag, and rolling features...\n",
            "🚀 Training model (Gradient Boosting)...\n",
            "✅ Validation MAE: 3.370\n",
            "💾 Model saved to models/model.joblib\n"
          ]
        }
      ]
    }
  ]
}