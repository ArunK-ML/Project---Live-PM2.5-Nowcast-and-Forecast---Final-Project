{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtG139oydpX/BCwurZuFdy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArunK-ML/Project---Live-PM2.5-Nowcast-and-Forecast---Final-Project/blob/main/PM_2_0_BEST_APPROCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from sklearn.ensemble import IsolationForest, GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# ==========================================================\n",
        "# CONFIG\n",
        "# ==========================================================\n",
        "TZ = timezone.utc\n",
        "# Default location (Chennai). Change to desired lat/lon.\n",
        "LAT, LON = 13.0827, 80.2707\n",
        "HOURS_BACK = 72\n",
        "RADIUS_KM = 30\n",
        "\n",
        "# ==========================================================\n",
        "# 1️⃣ DATA FETCHING\n",
        "# ==========================================================\n",
        "\n",
        "def fetch_openaq_pm25(lat: float, lon: float, hours: int = 168, radius_km: int = 30) -> pd.DataFrame:\n",
        "    \"\"\"Fetch PM2.5 measurements from OpenAQ and return hourly median with a 'timestamp' column (UTC).\"\"\"\n",
        "    end = datetime.now(TZ)\n",
        "    start = end - timedelta(hours=hours)\n",
        "    base = \"https://api.openaq.org/v2/measurements\"\n",
        "    params = {\n",
        "        \"coordinates\": f\"{lat},{lon}\",\n",
        "        \"radius\": int(radius_km * 1000),\n",
        "        \"parameter\": \"pm25\",\n",
        "        \"date_from\": start.isoformat(),\n",
        "        \"date_to\": end.isoformat(),\n",
        "        \"limit\": 10000,\n",
        "        \"sort\": \"desc\",\n",
        "        \"order_by\": \"datetime\",\n",
        "        \"page\": 1,\n",
        "    }\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            r = requests.get(base, params=params, timeout=30)\n",
        "            if r.status_code >= 400:\n",
        "                break\n",
        "            js = r.json()\n",
        "            items = js.get(\"results\", [])\n",
        "            if not items:\n",
        "                break\n",
        "            df = pd.DataFrame(items)\n",
        "            if \"date\" not in df:\n",
        "                break\n",
        "            # extract UTC datetime\n",
        "            df[\"timestamp\"] = pd.to_datetime(df[\"date\"].apply(lambda d: d.get(\"utc\")), utc=True)\n",
        "            df = df[[\"timestamp\", \"value\"]].rename(columns={\"value\": \"pm25\"})\n",
        "            frames.append(df)\n",
        "            meta = js.get(\"meta\", {})\n",
        "            found = meta.get(\"found\")\n",
        "            page = params[\"page\"]\n",
        "            limit = params[\"limit\"]\n",
        "            if found is None or page * limit >= int(found):\n",
        "                break\n",
        "            params[\"page\"] = page + 1\n",
        "    except Exception:\n",
        "        frames = []\n",
        "\n",
        "    if not frames:\n",
        "        return pd.DataFrame(columns=[\"timestamp\", \"pm25\"])\n",
        "\n",
        "    df = pd.concat(frames, ignore_index=True)\n",
        "    df = df[(df[\"timestamp\"] >= start) & (df[\"timestamp\"] <= end)]\n",
        "\n",
        "    # hourly median across stations\n",
        "    df_hour = (\n",
        "        df.set_index(\"timestamp\")\n",
        "          .groupby(pd.Grouper(freq=\"1H\"))[\"pm25\"]\n",
        "          .median()\n",
        "          .reset_index()\n",
        "    )\n",
        "    # ensure sorted\n",
        "    df_hour = df_hour.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    return df_hour\n",
        "\n",
        "def fetch_openmeteo_aq_pm25(lat: float, lon: float, hours: int = 168) -> pd.DataFrame:\n",
        "    \"\"\"Fallback: open-meteo air-quality PM2.5 hourly. Returns 'timestamp' and 'pm25' columns (UTC).\"\"\"\n",
        "    end = datetime.now(TZ)\n",
        "    start = end - timedelta(hours=hours)\n",
        "    # past_days param uses days; request a few days to cover hours window\n",
        "    past_days = max(1, (hours // 24) + 1)\n",
        "    url = (\n",
        "        \"https://air-quality-api.open-meteo.com/v1/air-quality?\"\n",
        "        f\"latitude={lat}&longitude={lon}&hourly=pm2_5&past_days={past_days}&timezone=UTC\"\n",
        "    )\n",
        "    try:\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        h = r.json().get(\"hourly\", {})\n",
        "        if not h:\n",
        "            return pd.DataFrame(columns=[\"timestamp\", \"pm25\"])\n",
        "        df = pd.DataFrame({\"timestamp\": h.get(\"time\", []), \"pm25\": h.get(\"pm2_5\", [])})\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
        "        df = df[(df[\"timestamp\"] >= start) & (df[\"timestamp\"] <= end)].reset_index(drop=True)\n",
        "        return df\n",
        "    except Exception:\n",
        "        return pd.DataFrame(columns=[\"timestamp\", \"pm25\"])\n",
        "\n",
        "def fetch_openmeteo_weather(lat: float, lon: float, hours: int = 168) -> pd.DataFrame:\n",
        "    \"\"\"Fetch meteorological hourly features from Open-Meteo (temperature, humidity, windspeed).\"\"\"\n",
        "    end = datetime.now(TZ)\n",
        "    start = end - timedelta(hours=hours)\n",
        "    past_days = max(1, (hours // 24) + 1)\n",
        "    url = (\n",
        "        \"https://api.open-meteo.com/v1/forecast?\"\n",
        "        f\"latitude={lat}&longitude={lon}&hourly=temperature_2m,relativehumidity_2m,windspeed_10m\"\n",
        "        f\"&past_days={past_days}&timezone=UTC\"\n",
        "    )\n",
        "    try:\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        j = r.json().get(\"hourly\", {})\n",
        "        if not j:\n",
        "            return pd.DataFrame(columns=[\"timestamp\"])\n",
        "        df = pd.DataFrame({\n",
        "            \"timestamp\": j.get(\"time\", []),\n",
        "            \"temperature_2m\": j.get(\"temperature_2m\", []),\n",
        "            \"relativehumidity_2m\": j.get(\"relativehumidity_2m\", []),\n",
        "            \"windspeed_10m\": j.get(\"windspeed_10m\", []),\n",
        "        })\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
        "        return df[(df[\"timestamp\"] >= start) & (df[\"timestamp\"] <= end)].reset_index(drop=True)\n",
        "    except Exception:\n",
        "        return pd.DataFrame(columns=[\"timestamp\"])\n",
        "\n",
        "# ==========================================================\n",
        "# 2️⃣ MERGE + PREPROCESSING\n",
        "# ==========================================================\n",
        "\n",
        "def merge_datasets(pm_df: pd.DataFrame, weather_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merge PM2.5 and weather data by timestamp (asof-merge).\n",
        "    If weather_df is empty, returns pm_df with timestamp/pm25 only.\n",
        "    \"\"\"\n",
        "    print(\"🔗 Merging datasets...\")\n",
        "    pm = pm_df.copy()\n",
        "    pm = pm.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    if weather_df is None or weather_df.empty:\n",
        "        return pm\n",
        "    w = weather_df.copy().sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    # asof merge: nearest earlier weather observation for each pm timestamp\n",
        "    merged = pd.merge_asof(pm, w, on=\"timestamp\", direction=\"nearest\", tolerance=pd.Timedelta(\"1H\"))\n",
        "    # if some weather cols are missing due to tolerance, that's okay\n",
        "    return merged\n",
        "\n",
        "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean and preprocess merged dataset. Ensures hourly frequency, interpolates missing values.\"\"\"\n",
        "    print(\"🧹 Cleaning data...\")\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    # ensure timestamp is datetime tz-aware\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
        "    df = df.drop_duplicates(subset=[\"timestamp\"])\n",
        "    df = df.set_index(\"timestamp\").sort_index()\n",
        "    # resample to 1H and take mean for numeric columns\n",
        "    df = df.resample(\"1H\").mean()\n",
        "    # interpolate small gaps, then forward/backward fill remaining\n",
        "    df = df.interpolate(limit=3).ffill().bfill()\n",
        "    df = df.reset_index()\n",
        "    return df\n",
        "\n",
        "def remove_outliers(df: pd.DataFrame, cols=None, z_thresh=3) -> pd.DataFrame:\n",
        "    \"\"\"Remove statistical outliers by z-score (replace with NaN -> interpolate).\"\"\"\n",
        "    print(\"🚫 Removing outliers...\")\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    if cols is None:\n",
        "        cols = [c for c in df.columns if c not in (\"timestamp\",)]\n",
        "    for c in cols:\n",
        "        if c not in df.columns or not np.issubdtype(df[c].dtype, np.number):\n",
        "            continue\n",
        "        std = df[c].std()\n",
        "        mean = df[c].mean()\n",
        "        if std == 0 or np.isnan(std):\n",
        "            continue\n",
        "        z = np.abs((df[c] - mean) / std)\n",
        "        df.loc[z > z_thresh, c] = np.nan\n",
        "    # interpolate and fill\n",
        "    df = df.interpolate(limit=6).ffill().bfill()\n",
        "    return df\n",
        "\n",
        "# ==========================================================\n",
        "# 3️⃣ FEATURE ENGINEERING\n",
        "# ==========================================================\n",
        "\n",
        "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df: pd.DataFrame, col=\"pm25\", lags=6) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for i in range(1, lags + 1):\n",
        "        df[f\"{col}_lag_{i}\"] = df[col].shift(i)\n",
        "    return df\n",
        "\n",
        "def add_rolling_features(df: pd.DataFrame, col=\"pm25\", windows=[3,6,12]) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for w in windows:\n",
        "        df[f\"{col}_roll_mean_{w}\"] = df[col].rolling(w, min_periods=1).mean()\n",
        "        df[f\"{col}_roll_std_{w}\"] = df[col].rolling(w, min_periods=1).std().fillna(0)\n",
        "    return df\n",
        "\n",
        "def feature_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    print(\"🧠 Creating time, lag, and rolling features...\")\n",
        "    df = df.copy()\n",
        "    # ensure pm25 exists\n",
        "    if \"pm25\" not in df.columns:\n",
        "        raise KeyError(\"pm25 column not found in dataframe (needed for feature engineering).\")\n",
        "    df = add_time_features(df)\n",
        "    df = add_lag_features(df)\n",
        "    df = add_rolling_features(df)\n",
        "    # drop rows with NaNs introduced by lagging\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# ==========================================================\n",
        "# 4️⃣ ANOMALY DETECTION\n",
        "# ==========================================================\n",
        "\n",
        "def detect_anomalies(df: pd.DataFrame, col=\"pm25\") -> pd.DataFrame:\n",
        "    print(\"⚠️ Detecting anomalies (IsolationForest)...\")\n",
        "    df = df.copy()\n",
        "    if col not in df.columns or df.empty:\n",
        "        df[\"anomaly_flag\"] = 0\n",
        "        return df\n",
        "    # IsolationForest expects 2D numeric input and no NaNs\n",
        "    X = df[[col]].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "    model = IsolationForest(contamination=0.05, random_state=42)\n",
        "    flags = model.fit_predict(X)\n",
        "    df[\"anomaly_flag\"] = np.where(flags == -1, 1, 0)\n",
        "    return df\n",
        "\n",
        "# ==========================================================\n",
        "# 5️⃣ TRAINING\n",
        "# ==========================================================\n",
        "\n",
        "def train_model(df: pd.DataFrame, model_out=\"models/model.joblib\"):\n",
        "    \"\"\"Train Gradient Boosting model.\"\"\"\n",
        "    os.makedirs(os.path.dirname(model_out), exist_ok=True)\n",
        "\n",
        "    df = df.copy()\n",
        "    df = feature_pipeline(df)\n",
        "\n",
        "    # drop non-feature columns\n",
        "    drop_cols = [\"timestamp\", \"anomaly_flag\"]\n",
        "    X = df.drop(columns=[c for c in drop_cols if c in df.columns] + [\"pm25\"], errors=\"ignore\")\n",
        "    y = df[\"pm25\"]\n",
        "\n",
        "    # simple chronological split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    print(\"🚀 Training model (Gradient Boosting)...\")\n",
        "    model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    preds = model.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, preds)\n",
        "    print(f\"✅ Validation MAE: {mae:.3f}\")\n",
        "\n",
        "    joblib.dump(model, model_out)\n",
        "    print(f\"💾 Model saved to {model_out}\")\n",
        "\n",
        "    return model, mae\n",
        "\n",
        "# ==========================================================\n",
        "# 6️⃣ MAIN PIPELINE\n",
        "# ==========================================================\n",
        "\n",
        "def main():\n",
        "    os.makedirs(\"data/processed\", exist_ok=True)\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "    # Fetch PM2.5 (OpenAQ preferred, fallback open-meteo)\n",
        "    pm_df = fetch_openaq_pm25(LAT, LON, hours=HOURS_BACK, radius_km=RADIUS_KM)\n",
        "    if pm_df.empty:\n",
        "        print(\"No OpenAQ data found — falling back to Open-Meteo air quality.\")\n",
        "        pm_df = fetch_openmeteo_aq_pm25(LAT, LON, hours=HOURS_BACK)\n",
        "\n",
        "    if pm_df.empty:\n",
        "        raise RuntimeError(\"Failed to fetch any PM2.5 data. Check network / API availability.\")\n",
        "\n",
        "    # Fetch weather features (temperature, humidity, wind)\n",
        "    weather_df = fetch_openmeteo_weather(LAT, LON, hours=HOURS_BACK)\n",
        "\n",
        "    merged_df = merge_datasets(pm_df, weather_df)\n",
        "    cleaned_df = clean_data(merged_df)\n",
        "    cleaned_df = remove_outliers(cleaned_df)\n",
        "    cleaned_df = detect_anomalies(cleaned_df)\n",
        "\n",
        "    processed_path = \"data/processed/pm25_merged.csv\"\n",
        "    cleaned_df.to_csv(processed_path, index=False)\n",
        "    print(f\"📁 Processed data saved to {processed_path}\")\n",
        "\n",
        "    # Train model\n",
        "    train_model(cleaned_df, model_out=\"models/model.joblib\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At3FXFkWMRvS",
        "outputId": "199f82a8-14b7-4170-fe2a-e9201df4746c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No OpenAQ data found — falling back to Open-Meteo air quality.\n",
            "🔗 Merging datasets...\n",
            "🧹 Cleaning data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-929650355.py:150: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  merged = pd.merge_asof(pm, w, on=\"timestamp\", direction=\"nearest\", tolerance=pd.Timedelta(\"1H\"))\n",
            "/tmp/ipython-input-929650355.py:165: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  df = df.resample(\"1H\").mean()\n",
            "/tmp/ipython-input-929650355.py:239: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X = df[[col]].fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚫 Removing outliers...\n",
            "⚠️ Detecting anomalies (IsolationForest)...\n",
            "📁 Processed data saved to data/processed/pm25_merged.csv\n",
            "🧠 Creating time, lag, and rolling features...\n",
            "🚀 Training model (Gradient Boosting)...\n",
            "✅ Validation MAE: 3.370\n",
            "💾 Model saved to models/model.joblib\n"
          ]
        }
      ]
    }
  ]
}